{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "# from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "# be able to save images on server\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# !pip install -U kaleido # w Google Colab wymagany Runtime restart po instalacji (Runtime -> Restart Runtime)\n",
    "# import kaleido #required\n",
    "# kaleido.__version__ #0.2.1\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip\n",
    "# importing the \"tarfile\" module\n",
    "import tarfile # type: ignore\n",
    "\n",
    "# open file\n",
    "file = tarfile.open('1_150x9.tar.gz')\n",
    "\n",
    "# extracting a specific file\n",
    "file.extractall(path='./1_150x9/')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator\n",
    "def data():\n",
    "    timestepsPerSample = 150\n",
    "\n",
    "    # Replace 'your_file.csv' with the actual file path\n",
    "    file_features = './1_150x9/1_150x9f.csv'\n",
    "    file_labels = './1_150x9/1_150x9l.csv'\n",
    "\n",
    "    # Read the .csv file and create an array\n",
    "    data_strings = np.genfromtxt(file_features, delimiter=';')\n",
    "    labels_strings = np.genfromtxt(file_labels,delimiter=';')\n",
    "\n",
    "    # Wycinanie wybranych kolumn\n",
    "    # data_s = data_strings[:,[0,2,3,4,5,6,7,8]]\n",
    "    data_s = data_strings[:,[0,4,5,8]]\n",
    "    num_features = data_s.shape[1]\n",
    "    # print(data_strings[:3])\n",
    "    # print(data_s[:3])\n",
    "    \n",
    "    # Convert from strings to float and int\n",
    "    X = data_s.astype(float).reshape((-1,timestepsPerSample,num_features))\n",
    "    Y = labels_strings.astype(float).reshape((-1,3))\n",
    "    # print(X.shape)\n",
    "    # print(Y.shape)\n",
    "\n",
    "    # Modification from imported to new size\n",
    "    X_mod = X[1000:11000,130:]\n",
    "    Y_mod = Y[1000:11000]\n",
    "    timestepsPerSample = X_mod.shape[1]\n",
    "    # print(X_mod.shape)\n",
    "    # print(Y_mod.shape)\n",
    "    # print(X_mod[:1])\n",
    "\n",
    "    # Splitting\n",
    "    x_train, x_test, Y_train, Y_test = train_test_split(X_mod,Y_mod, test_size=0.15, shuffle=False)\n",
    "    y_train = Y_train[:, 0:2]\n",
    "    y_test = Y_test[:, 0:2]\n",
    "    # print(y_train[:2])\n",
    "    # print(y_test[:2])\n",
    "\n",
    "    # Summarize\n",
    "    num_samples = x_train.shape[0]\n",
    "    test_samples = x_test.shape[0]\n",
    "    print(f\"Train shape: {x_train.shape}\")\n",
    "    print(f\"Timesteps: {timestepsPerSample}\")\n",
    "    print(f\"Num Samples: {num_samples}\")\n",
    "    print(f\"Test Samples: {test_samples}\")\n",
    "    print(f\"Num features: {num_features}\")\n",
    "    return x_train, x_test, y_train, y_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit lstm model\n",
    "def fit_lstmModel(x_train, y_train, x_test, y_test, batch_size, nb_epoch, neurons):\n",
    "    LstmLayer = LSTM(\n",
    "    units=neurons,\n",
    "    activation=\"tanh\",\n",
    "    recurrent_activation=\"sigmoid\",\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    recurrent_initializer=\"orthogonal\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    unit_forget_bias=True,\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.0,\n",
    "    seed=None,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    "    unroll=False,\n",
    "    input_shape=(x_train.shape[1],x_train.shape[2])\n",
    "    )\n",
    "\n",
    "    num_samples = x_train.shape[0]\n",
    "    STEPS_PER_EPOCH = num_samples/batch_size\n",
    "\n",
    "    model = Sequential() # initializing model\n",
    "    # input layer and LSTM layer with 50 neurons\n",
    "    model.add(LstmLayer)\n",
    "    # model.add(Dense(100, activation='relu'))\n",
    "    # model.add(Dense(100, activation='relu'))\n",
    "    # model.add(Dense(20, activation='relu'))\n",
    "    # outpute layer with sigmoid activation\n",
    "    model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "    # lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    0.001,\n",
    "    decay_steps=STEPS_PER_EPOCH*100,\n",
    "    decay_rate=0.9,\n",
    "    staircase=False)\n",
    "\n",
    "    def get_optimizer():\n",
    "         # return tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        return tf.keras.optimizers.Adam(learning_rate= lr_schedule) # type: ignore\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "\n",
    "    # defining loss function, optimizer, metrics and then compiling model\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit(x_train, y_train, epochs=nb_epoch, batch_size=batch_size, \n",
    "                        shuffle=False, validation_data=(x_test, y_test), verbose=1) # type: ignore\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profit\n",
    "def funcProfit(predict, Y_test):\n",
    "    predict_classes = np.where(predict > 0.5, 1,0)\n",
    "    concat = np.hstack((predict_classes, Y_test[:,2:]))\n",
    "    df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Price'])\n",
    "\n",
    "    # Absolute difference prices\n",
    "    sum = 0\n",
    "    for i in range(0, len(df)):\n",
    "        if (i-1) > 0:\n",
    "            diff = abs((df.at[i,'Price'] - df.at[i-1,'Price']))\n",
    "            if diff < 0.5:\n",
    "                if df.at[i,'Sell'] == 1:\n",
    "                    sum += (df.at[i,'Price'] - df.at[i-1,'Price'])*(-1)\n",
    "                elif df.at[i,'Buy'] == 1:\n",
    "                    sum += (df.at[i,'Price'] - df.at[i-1,'Price'])\n",
    "                if df.at[i,'Sell'] != df.at[i-1,'Sell']:\n",
    "                    sum -= 0.03\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment\n",
    "def experiment(repeats, epochs):\n",
    "    # Data gen\n",
    "    x_train, x_test, y_train, y_test, Y_test = data()\n",
    "\n",
    "    # Evaluate declaration\n",
    "    accuracy = list()\n",
    "    profit = list()\n",
    "    metrics = list()\n",
    "\n",
    "    # Repeats\n",
    "    for r in range(repeats):\n",
    "        batch_size = 4\n",
    "        # model\n",
    "        model, history = fit_lstmModel(x_train, y_train, x_test, y_test, batch_size, epochs, 5)\n",
    "\n",
    "        # forecast the entire training dataset to build up state for forecasting\n",
    "        predict = model.predict(x_test, batch_size=batch_size)\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0) # type: ignore\n",
    "\n",
    "        metric = pd.DataFrame(history.history)\n",
    "        metric['epoch'] = history.epoch\n",
    "\n",
    "        metrics.append(metric)\n",
    "        accuracy.append(test_acc*100)\n",
    "        profit.append(funcProfit(predict, Y_test))\n",
    "    \n",
    "    return accuracy, profit, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "from numpy import dtype\n",
    "\n",
    "repeats = 1\n",
    "results_acc = DataFrame(dtype='float')\n",
    "results_profit = DataFrame(dtype='float')\n",
    "metrics = list()\n",
    "\n",
    "# vary training epochs\n",
    "epochs = [1]\n",
    "for e in epochs:\n",
    " results_acc[str(e)], results_profit[str(e)], metrics = experiment(repeats, e)\n",
    "\n",
    "# summarize results\n",
    "print(results_acc.describe())\n",
    "print(results_profit.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "for idx, m in enumerate(metrics):\n",
    "    fig.add_trace(go.Scatter(x=m['epoch'], y=m['accuracy'], name=f'accuracy{str(idx)}', line_color='#0000ff', showlegend=False), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=m['epoch'], y=m['loss'], name=f'loss{str(idx)}', line_color='#0000ff', showlegend=False), row=1, col=2)\n",
    "    fig.add_trace(go.Scatter(x=m['epoch'], y=m['val_accuracy'], name=f'val_accuracy{str(idx)}', line_color='#EF8260', showlegend=False), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=m['epoch'], y=m['val_loss'], name=f'val_loss{str(idx)}', line_color='#EF8260', showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text='epochs')\n",
    "fig.update_yaxes(title_text='accuracy')\n",
    "fig.update_layout(width=1000, title='Accuracy and Loss')\n",
    "fig.write_image(file=f\"compare_models_{str(epochs)}.jpg\", engine=\"kaleido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(results_acc.describe())\n",
    "\n",
    "# 2 subplots in one row\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "# Generate boxplots\n",
    "results_acc.boxplot(ax=ax[0])\n",
    "results_profit.boxplot(ax=ax[1])\n",
    "\n",
    "# Set labels and titles\n",
    "ax[0].set_title('Accuracy')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Profit')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Profit')\n",
    "\n",
    "# Save to .png and show plot\n",
    "plt.savefig(f'boxplot_{str(epochs)}.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
