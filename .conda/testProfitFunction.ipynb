{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intallation packages before use Jupyter\n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install keras\n",
    "# !pip install matplotlib\n",
    "# !pip install kaleido\n",
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "# from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "from math import sqrt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "# be able to save images on server\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Not show warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# !pip install -U kaleido # w Google Colab wymagany Runtime restart po instalacji (Runtime -> Restart Runtime)\n",
    "# import kaleido #required\n",
    "# kaleido.__version__ #0.2.1\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip\n",
    "# importing the \"tarfile\" module\n",
    "import tarfile # type: ignore\n",
    "\n",
    "# open file\n",
    "file = tarfile.open('../data/2_150x9.tar.gz')\n",
    "\n",
    "# extracting a specific file\n",
    "file.extractall(path='../data/2_150x9/')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator\n",
    "def data(time):\n",
    "    # Timestep \n",
    "    # timestepsPerSample = 20\n",
    "    timestepsPerSample = time\n",
    "\n",
    "    # Timesteps in input data\n",
    "    timestepsPerSampleWholeData = 150\n",
    "\n",
    "    # Replace 'your_file.csv' with the actual file path\n",
    "    file_features = '../data/2_150x9/2_150x9f.csv'\n",
    "    file_labels = '../data/2_150x9/2_150x9l.csv'\n",
    "\n",
    "    # Read the .csv file and create an array\n",
    "    data_strings = np.genfromtxt(file_features, delimiter=';')\n",
    "    labels_strings = np.genfromtxt(file_labels,delimiter=';')\n",
    "\n",
    "    # Wycinanie wybranych kolumn\n",
    "    #  0    1         2             3        4       5        6    7        8\n",
    "    # RSI, VWAP, HeikenResult, closeHeiken, CMF, Stochastic, OBV, QQE, TrendFilter\n",
    "    # data_s = data_strings[:,[0,2,3,4,5,6,7,8]]\n",
    "    data_s = data_strings[:,[0,4,5,8]]\n",
    "    num_features = data_s.shape[1]\n",
    "    # print(data_strings[:3])\n",
    "    # print(data_s[:3])\n",
    "    \n",
    "    # Convert from strings to float and int\n",
    "    X = data_s.astype(float).reshape((-1,timestepsPerSampleWholeData,num_features))\n",
    "    Y = labels_strings.astype(float).reshape((-1,6))\n",
    "    # print(X.shape)\n",
    "    # print(Y.shape)\n",
    "\n",
    "    # Modification from imported to new size X[samples,timesteps]\n",
    "    X_mod = X[1000:1020,timestepsPerSampleWholeData - timestepsPerSample:]\n",
    "    Y_mod = Y[1000:1020]\n",
    "    timestepsPerSampleWholeData = X_mod.shape[1]\n",
    "    # print(X_mod.shape)\n",
    "    # print(Y_mod.shape)\n",
    "    # print(X_mod[:1])\n",
    "\n",
    "    # Splitting\n",
    "    x_train, x_test, Y_train, Y_test = train_test_split(X_mod,Y_mod, test_size=0.15, shuffle=False)\n",
    "    y_train = Y_train[:, 0:2]\n",
    "    y_test = Y_test[:, 0:2]\n",
    "    # print(y_train[:2])\n",
    "    # print(y_test[:2])\n",
    "\n",
    "    # Summarize\n",
    "    num_samples = x_train.shape[0]\n",
    "    test_samples = x_test.shape[0]\n",
    "    print(f\"Train shape: {x_train.shape}\")\n",
    "    print(f\"Timesteps: {timestepsPerSampleWholeData}\")\n",
    "    print(f\"Num Samples: {num_samples}\")\n",
    "    print(f\"Test Samples: {test_samples}\")\n",
    "    print(f\"Num features: {num_features}\")\n",
    "    return x_train, x_test, y_train, y_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit lstm model\n",
    "from tabnanny import verbose\n",
    "\n",
    "\n",
    "def fit_lstmModel(x_train, y_train, x_test, y_test, batch_size, nb_epoch, neurons, denseType, dropout, model):\n",
    "    if model == 0:\n",
    "        LstmLayer = LSTM(\n",
    "        units=neurons,\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "        use_bias=True, # true if cuDNN\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        recurrent_initializer=\"orthogonal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        unit_forget_bias=True,\n",
    "        kernel_regularizer=None,\n",
    "        recurrent_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        recurrent_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        dropout=dropout, # !important parameter for optimization => 0 if cuDNN\n",
    "        recurrent_dropout=0.0,\n",
    "        seed=None,\n",
    "        return_sequences=False,\n",
    "        return_state=False,\n",
    "        go_backwards=False,\n",
    "        stateful=False,\n",
    "        unroll=False, # false if cuDNN\n",
    "        input_shape=(x_train.shape[1],x_train.shape[2]),\n",
    "        # input_dim=(x_train.shape[1])\n",
    "        )\n",
    "\n",
    "        if denseType == 0:\n",
    "            model = Sequential() # initializing model\n",
    "            # input layer and LSTM layer with 50 neurons\n",
    "            model.add(LstmLayer)\n",
    "            # model.add(Dense(100, activation='relu'))\n",
    "            # model.add(Dense(100, activation='relu'))\n",
    "            # model.add(Dense(20, activation='relu'))\n",
    "            # outpute layer with sigmoid activation\n",
    "            model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "        elif denseType == 1:\n",
    "            model = Sequential() # initializing model\n",
    "            # input layer and LSTM layer with 50 neurons\n",
    "            model.add(LstmLayer)\n",
    "            model.add(Dense(100, activation='relu'))\n",
    "            # model.add(Dense(100, activation='relu'))\n",
    "            # model.add(Dense(20, activation='relu'))\n",
    "            # outpute layer with sigmoid activation\n",
    "            model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "        elif denseType == 2:\n",
    "            model = Sequential() # initializing model\n",
    "            # input layer and LSTM layer with 50 neurons\n",
    "            model.add(LstmLayer)\n",
    "            model.add(Dense(100, activation='relu'))\n",
    "            model.add(Dense(100, activation='relu'))\n",
    "            # model.add(Dense(20, activation='relu'))\n",
    "            # outpute layer with sigmoid activation\n",
    "            model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "        elif denseType == 3:\n",
    "            model = Sequential() # initializing model\n",
    "            # input layer and LSTM layer with 50 neurons\n",
    "            model.add(LstmLayer)\n",
    "            model.add(Dense(100, activation='relu'))\n",
    "            model.add(Dense(100, activation='relu'))\n",
    "            model.add(Dense(20, activation='relu'))\n",
    "            # outpute layer with sigmoid activation\n",
    "            model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "    elif model == 1:\n",
    "        model = Sequential()\n",
    "\n",
    "        # Assuming `data` is your input matrix with shape (samples, time_steps, features)\n",
    "        model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(LSTM(units=50, return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(units=25))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # Output layer for price prediction\n",
    "        model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    num_samples = x_train.shape[0]\n",
    "    STEPS_PER_EPOCH = num_samples/batch_size\n",
    "\n",
    "    # lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    0.001,\n",
    "    decay_steps=STEPS_PER_EPOCH*100,\n",
    "    decay_rate=0.9,\n",
    "    staircase=False)\n",
    "\n",
    "    # Callbackks \n",
    "    # EarlyStopping\n",
    "    checkpoint = ModelCheckpoint(\n",
    "            filepath='../saved_models/last_saved_model.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_loss',\n",
    "            verbose=1\n",
    "            )\n",
    "    # checkpoint = ModelCheckpoint('model.h5', save_best_only=True, save_format='h5', verbose=1)\n",
    "    earlyStoppingCallback = EarlyStopping(monitor='val_loss',\n",
    "                                             start_from_epoch=10,\n",
    "                                             restore_best_weights=True,\n",
    "                                             verbose=0,\n",
    "                                             patience=5)\n",
    "\n",
    "    def get_optimizer():\n",
    "         # return tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        return tf.keras.optimizers.Adam(learning_rate= lr_schedule) # type: ignore\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "\n",
    "    # defining loss function, optimizer, metrics and then compiling model\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit(x_train, y_train, epochs=nb_epoch, batch_size=batch_size, \n",
    "                        shuffle=False, validation_data=(x_test, y_test), callbacks=[checkpoint, earlyStoppingCallback], verbose=2) # type: ignore\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (17, 150, 4)\n",
      "Timesteps: 150\n",
      "Num Samples: 17\n",
      "Test Samples: 3\n",
      "Num features: 4\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, Y_test = data(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing iris dataset from sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "loaded_model = load_model(filepath=\"./saved_models/last_saved_model.keras\") \n",
    "predict = loaded_model.predict(x_test)\n",
    "predict_classes = np.where(predict > 0.5, 1,0)\n",
    "concat = np.hstack((predict_classes, Y_test[:,2:]))\n",
    "concat2 = np.hstack((concat, y_test))\n",
    "df = pd.DataFrame(concat2, columns=['Sell', 'Buy', 'Close', 'Open', 'High', 'Low', 'Sell_org', 'Buy_org'])\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 679ms/step\n",
      "[[0.52603877 0.5570794 ]\n",
      " [0.5319065  0.55717134]\n",
      " [0.539512   0.5577674 ]]\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "[[ 1.    1.   83.27 83.34 83.34 83.26]\n",
      " [ 1.    1.   83.27 83.28 83.31 83.26]\n",
      " [ 1.    1.   83.28 83.27 83.29 83.25]]\n",
      "[[ 1.    0.   83.27 83.34 83.34 83.26]\n",
      " [ 1.    0.   83.27 83.28 83.31 83.26]\n",
      " [ 1.    0.   83.28 83.27 83.29 83.25]]\n",
      "   Sell  Buy  Close   Open   High    Low\n",
      "0   1.0  1.0  83.27  83.34  83.34  83.26\n",
      "1   1.0  1.0  83.27  83.28  83.31  83.26\n",
      "2   1.0  1.0  83.28  83.27  83.29  83.25\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model(filepath=\"../saved_models/last_saved_model.keras\") \n",
    "# loaded_model.summary()\n",
    "predict = loaded_model.predict(x_test)\n",
    "print(predict)\n",
    "predict_classes = np.where(predict > 0.5, 1,0)\n",
    "print(predict_classes)\n",
    "concat = np.hstack((predict_classes, Y_test[:,2:]))\n",
    "print(concat)\n",
    "print(Y_test)\n",
    "df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Close', 'Open', 'High', 'Low'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sell  Buy  Close  Open  High  Low\n",
      "0    1.0  0.0   1.00  1.00   1.0  1.0\n",
      "1    1.0  0.0   1.00  1.00   1.0  1.0\n",
      "2    1.0  0.0   0.75  1.00   1.0  1.0\n",
      "3    1.0  0.0   0.50  0.75   1.0  1.0\n",
      "4    0.0  1.0   0.50  0.50   1.0  1.0\n",
      "5    0.0  1.0   1.00  0.50   1.0  1.0\n",
      "6    0.0  1.0   1.00  1.00   1.0  1.0\n",
      "7    1.0  0.0   1.00  1.00   1.0  1.0\n",
      "8    1.0  0.0   1.00  1.00   1.0  1.0\n",
      "9    1.0  0.0   1.00  1.00   1.0  1.0\n",
      "10   0.0  1.0   1.00  1.20   1.0  1.0\n",
      "11   0.0  1.0   1.00  1.00   1.0  1.0\n",
      "0.24\n",
      "0.9099999999999999\n"
     ]
    }
   ],
   "source": [
    "# data = np.array([[1, 0, 1, 1, 1, 1],\n",
    "#                  [1, 0, 1, 1, 1, 1],\n",
    "#                  [1, 0, 1, 1, 1, 1],\n",
    "#                  [1, 0, 0.5, 0.5, 1, 1],\n",
    "#                  [0, 1, 0.5, 0.5, 1, 1],\n",
    "#                  [0, 1, 1, 1, 1, 1],\n",
    "#                  [0, 1, 1, 1, 1, 1],\n",
    "#                  [1, 0, 1, 1, 1, 1],\n",
    "#                  [1, 0, 1, 1, 1, 1]\n",
    "#                  ])\n",
    "data = np.array([[1, 0, 1, 1, 1, 1],\n",
    "                 [1, 0, 1, 1, 1, 1],\n",
    "                 [1, 0, 0.75, 1, 1, 1],\n",
    "                 [1, 0, 0.5, 0.75, 1, 1],\n",
    "                 [0, 1, 0.5, 0.5, 1, 1],\n",
    "                 [0, 1, 1, 0.5, 1, 1],\n",
    "                 [0, 1, 1, 1, 1, 1],\n",
    "                 [1, 0, 1, 1, 1, 1],\n",
    "                 [1, 0, 1, 1, 1, 1],\n",
    "                 [1, 0, 1, 1, 1, 1],\n",
    "                 [0, 1, 1, 1.2, 1, 1],\n",
    "                 [0, 1, 1, 1, 1, 1]\n",
    "                 ])\n",
    "df = pd.DataFrame(data, columns=['Sell', 'Buy', 'Close', 'Open', 'High', 'Low'])\n",
    "print(df)\n",
    "print(funcProfitDf(df))\n",
    "print(funcProfitOldDf(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profit modified to use ready data\n",
    "# Profit old\n",
    "def funcProfitOldDf(df):\n",
    "    # predict_classes = np.where(predict > 0.5, 1,0)\n",
    "    # concat = np.hstack((predict_classes, Y_test[:,2:]))\n",
    "    # df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Price'])\n",
    "    # df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Close', 'Open', 'High', 'Low'])\n",
    "\n",
    "    # Absolute difference prices\n",
    "    sum = 0\n",
    "    for i in range(0, len(df)):\n",
    "        if (i-1) > 0:\n",
    "            if df.at[i,'Sell'] == df.at[i-1,'Sell']:\n",
    "                if df.at[i,'Sell'] == 1:\n",
    "                    sum += (df.at[i,'Close'] - df.at[i-1,'Close'])*(-1)\n",
    "                elif df.at[i,'Buy'] == 1:\n",
    "                    sum += (df.at[i,'Close'] - df.at[i-1,'Close'])\n",
    "            if df.at[i,'Sell'] == df.at[i-1,'Buy'] or df.at[i,'Sell'] == df.at[i-1,'Buy']:\n",
    "                sum -= 0.03\n",
    "    return sum\n",
    "\n",
    "\n",
    "# Profit\n",
    "def funcProfitDf(df):\n",
    "    # predict_classes = np.where(predict > 0.5, 1,0)\n",
    "    # concat = np.hstack((predict_classes, Y_test[:,2:]))\n",
    "    # # df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Price'])\n",
    "    # df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Close', 'Open', 'High', 'Low'])\n",
    "\n",
    "    # Absolute difference prices\n",
    "    spread = 0.03\n",
    "    tp = 1000 # 0.20\n",
    "    sum = 0\n",
    "    sell = 0\n",
    "    buy = 0\n",
    "    for i in range(0, len(df)):\n",
    "        if (i-1) >= 0:\n",
    "            if df.at[i,'Sell'] > 0.8 and df.at[i-1,'Sell'] < 0.2:                  # S 0 1 \n",
    "                if buy > 0:\n",
    "                    sell = df.at[i,'Open'] - spread\n",
    "                    sum += df.at[i,'Open'] - buy\n",
    "                    buy = 0\n",
    "                elif sell == 0:\n",
    "                    sell = df.at[i,'Open'] - spread\n",
    "            elif df.at[i,'Sell'] > 0.8 and df.at[i-1,'Sell'] > 0.8 and sell > 0:   # S 1 1 sell >0\n",
    "                if df.at[i-1,'High'] >= (sell + tp):\n",
    "                    sum -= tp\n",
    "                    sell = 0\n",
    "                if df.at[i-1,'Low'] <= (sell - tp):\n",
    "                    sum += tp\n",
    "                    sell = 0\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            elif df.at[i,'Buy'] > 0.8 and df.at[i-1,'Buy'] < 0.2:               # B 0 1 \n",
    "                if sell > 0:\n",
    "                    buy = df.at[i,'Open'] + spread\n",
    "                    sum += sell - df.at[i,'Open']\n",
    "                    sell = 0\n",
    "                elif buy == 0:\n",
    "                    buy = df.at[i,'Open'] + spread\n",
    "            elif df.at[i,'Buy'] > 0.8 and df.at[i-1,'Buy'] > 0.8 and buy > 0:   # B 1 1 buy >0\n",
    "                if df.at[i-1,'Low'] <= (buy - tp):\n",
    "                    sum -= tp\n",
    "                    buy = 0\n",
    "                if df.at[i-1,'High'] >= (buy + tp):\n",
    "                    sum += tp\n",
    "                    buy = 0\n",
    "\n",
    "    return round(sum, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profit old\n",
    "def funcProfitOld(predict, Y_test):\n",
    "    predict_classes = np.where(predict > 0.5, 1,0)\n",
    "    concat = np.hstack((predict_classes, Y_test[:,2:]))\n",
    "    # df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Price'])\n",
    "    df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Close', 'Open', 'High', 'Low'])\n",
    "\n",
    "    # Absolute difference prices\n",
    "    sum = 0\n",
    "    for i in range(0, len(df)):\n",
    "        if (i-1) > 0:\n",
    "            diff = abs((df.at[i,'Sell'] - df.at[i-1,'Sell']))\n",
    "            if diff < 0.5:\n",
    "                if df.at[i,'Sell'] == 1:\n",
    "                    sum += (df.at[i,'Close'] - df.at[i-1,'Close'])*(-1)\n",
    "                elif df.at[i,'Buy'] == 1:\n",
    "                    sum += (df.at[i,'Close'] - df.at[i-1,'Close'])\n",
    "                if df.at[i,'Sell'] != df.at[i-1,'Sell']:\n",
    "                    sum -= 0.03\n",
    "    return sum\n",
    "\n",
    "\n",
    "# Profit\n",
    "def funcProfit(predict, Y_test):\n",
    "    predict_classes = np.where(predict > 0.5, 1,0)\n",
    "    concat = np.hstack((predict_classes, Y_test[:,2:]))\n",
    "    # df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Price'])\n",
    "    df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Close', 'Open', 'High', 'Low'])\n",
    "\n",
    "    # Absolute difference prices\n",
    "    spread = 0.03\n",
    "    tp = 1000 # 0.20\n",
    "    sum = 0\n",
    "    sell = 0\n",
    "    buy = 0\n",
    "    for i in range(0, len(df)):\n",
    "        if (i-1) >= 0:\n",
    "            if df.at[i,'Sell'] > 0.8 and df.at[i-1,'Sell'] < 0.2:                  # S 0 1 \n",
    "                if buy > 0:\n",
    "                    sell = df.at[i,'Open'] - spread\n",
    "                    sum += df.at[i,'Open'] - buy\n",
    "                    buy = 0\n",
    "                elif sell == 0:\n",
    "                    sell = df.at[i,'Open'] - spread\n",
    "            elif df.at[i,'Sell'] > 0.8 and df.at[i-1,'Sell'] > 0.8 and sell > 0:   # S 1 1 sell >0\n",
    "                if df.at[i-1,'High'] >= (sell + tp):\n",
    "                    sum -= tp\n",
    "                    sell = 0\n",
    "                if df.at[i-1,'Low'] <= (sell - tp):\n",
    "                    sum += tp\n",
    "                    sell = 0\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            elif df.at[i,'Buy'] > 0.8 and df.at[i-1,'Buy'] < 0.2:               # B 0 1 \n",
    "                if sell > 0:\n",
    "                    buy = df.at[i,'Open'] + spread\n",
    "                    sum += sell - df.at[i,'Open']\n",
    "                    sell = 0\n",
    "                elif buy == 0:\n",
    "                    buy = df.at[i,'Open'] + spread\n",
    "            elif df.at[i,'Buy'] > 0.8 and df.at[i-1,'Buy'] > 0.8 and buy > 0:   # B 1 1 buy >0\n",
    "                if df.at[i-1,'Low'] <= (buy - tp):\n",
    "                    sum -= tp\n",
    "                    buy = 0\n",
    "                if df.at[i-1,'High'] >= (buy + tp):\n",
    "                    sum += tp\n",
    "                    buy = 0\n",
    "\n",
    "    return round(sum, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment\n",
    "def experiment(repeats, epochs, neurons, time, denseType, dropout, model):\n",
    "    # Data gen\n",
    "    x_train, x_test, y_train, y_test, Y_test = data(time)\n",
    "\n",
    "    # Evaluate declaration\n",
    "    accuracy = list()\n",
    "    profit = list()\n",
    "    metrics = list()\n",
    "\n",
    "    # Repeats\n",
    "    for r in range(repeats):\n",
    "        \n",
    "        # Print running\n",
    "        print(f\"Repeat {r} running...\")\n",
    "\n",
    "        # Batch size\n",
    "        batch_size = 64\n",
    "        # model\n",
    "        model, history = fit_lstmModel(x_train, y_train, x_test, y_test, batch_size, epochs, neurons, denseType, dropout, model)\n",
    "\n",
    "        # forecast the entire training dataset to build up state for forecasting\n",
    "        predict = model.predict(x_test, batch_size=batch_size)\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0) # type: ignore\n",
    "\n",
    "        metric = pd.DataFrame(history.history)\n",
    "        metric['epoch'] = history.epoch\n",
    "\n",
    "        metrics.append(metric)\n",
    "        accuracy.append(test_acc*100)\n",
    "        profit.append(funcProfit(predict, Y_test))\n",
    "    \n",
    "    return accuracy, profit, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "def plotsOut(d, metrics):\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=('Accuracy', 'Loss'))\n",
    "\n",
    "    # Layout - set size\n",
    "    fig.update_layout(\n",
    "        autosize=True,\n",
    "        width=1000\n",
    "    )\n",
    "\n",
    "    for idx, m in enumerate(metrics):\n",
    "        fig.add_trace(go.Scatter(x=m['epoch'], y=m['accuracy'], name=f'accuracy{str(idx)}', line_color='#0000ff', showlegend=False), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=m['epoch'], y=m['loss'], name=f'loss{str(idx)}', line_color='#0000ff', showlegend=False), row=1, col=2)\n",
    "        fig.add_trace(go.Scatter(x=m['epoch'], y=m['val_accuracy'], name=f'val_accuracy{str(idx)}', line_color='#EF8260', showlegend=False), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=m['epoch'], y=m['val_loss'], name=f'val_loss{str(idx)}', line_color='#EF8260', showlegend=False), row=1, col=2)\n",
    "\n",
    "    fig.update_xaxes(title_text='epochs')\n",
    "    fig.update_yaxes(title_text='')\n",
    "    # fig.update_layout(width=1000, title='Accuracy and Loss')\n",
    "    fig.write_image(file=f\"compare_models_{str(d)}.jpg\", engine=\"kaleido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Model test start...\n",
      "Train shape: (17, 150, 4)\n",
      "Timesteps: 150\n",
      "Num Samples: 17\n",
      "Test Samples: 3\n",
      "Num features: 4\n",
      "Repeat 0 running...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EarlyStopping.__init__() got an unexpected keyword argument 'start_from_epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m model:\n\u001b[0;32m     17\u001b[0m  \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Model test start...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m  results_acc[\u001b[38;5;28mstr\u001b[39m(mod)], results_profit[\u001b[38;5;28mstr\u001b[39m(mod)], metrics \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneuronsLstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeStep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenseType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#  plotsOut(mod, metrics)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# summarize results\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(results_acc\u001b[38;5;241m.\u001b[39mdescribe())\n",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m, in \u001b[0;36mexperiment\u001b[1;34m(repeats, epochs, neurons, time, denseType, dropout, model)\u001b[0m\n\u001b[0;32m     18\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# model\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mfit_lstmModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneurons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenseType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# forecast the entire training dataset to build up state for forecasting\u001b[39;00m\n\u001b[0;32m     23\u001b[0m predict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "Cell \u001b[1;32mIn[6], line 107\u001b[0m, in \u001b[0;36mfit_lstmModel\u001b[1;34m(x_train, y_train, x_test, y_test, batch_size, nb_epoch, neurons, denseType, dropout, model)\u001b[0m\n\u001b[0;32m    100\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[0;32m    101\u001b[0m         filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../saved_models/last_saved_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    102\u001b[0m         save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    103\u001b[0m         monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    104\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    105\u001b[0m         )\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# checkpoint = ModelCheckpoint('model.h5', save_best_only=True, save_format='h5', verbose=1)\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m earlyStoppingCallback \u001b[38;5;241m=\u001b[39m \u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mstart_from_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_optimizer\u001b[39m():\n\u001b[0;32m    114\u001b[0m      \u001b[38;5;66;03m# return tf.keras.optimizers.Adam(learning_rate=0.001)\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m lr_schedule) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: EarlyStopping.__init__() got an unexpected keyword argument 'start_from_epoch'"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "from numpy import dtype\n",
    "\n",
    "repeats = 1\n",
    "results_acc = DataFrame(dtype='float')\n",
    "results_profit = DataFrame(dtype='float')\n",
    "metrics = list()\n",
    "\n",
    "# vary training epochs\n",
    "model = [0]\n",
    "dropout = 0.2\n",
    "denseType = 0\n",
    "neuronsLstm = 150\n",
    "timeStep = 150\n",
    "epochs = 2\n",
    "for mod in model:\n",
    " print(f\"{mod} Model test start...\")\n",
    " results_acc[str(mod)], results_profit[str(mod)], metrics = experiment(repeats, epochs, neuronsLstm, timeStep, denseType, dropout, mod)\n",
    "#  plotsOut(mod, metrics)\n",
    "\n",
    "# summarize results\n",
    "print(results_acc.describe())\n",
    "print(results_profit.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(results_acc.describe())\n",
    "print(results_profit.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(results_acc.describe())\n",
    "\n",
    "# 2 subplots in one row\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "# Generate boxplots\n",
    "results_acc.boxplot(ax=ax[0])\n",
    "results_profit.boxplot(ax=ax[1])\n",
    "\n",
    "# Set labels and titles\n",
    "ax[0].set_title('Accuracy')\n",
    "ax[0].set_xlabel('Dropout')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Profit')\n",
    "ax[1].set_xlabel('Dropout')\n",
    "ax[1].set_ylabel('Profit')\n",
    "\n",
    "# Save to .png and show plot\n",
    "plt.savefig(f'boxplot_model.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
