{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intallation packages before use Jupyter\n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install keras\n",
    "# !pip install matplotlib\n",
    "# !pip install kaleido\n",
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "# from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from math import sqrt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "# be able to save images on server\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Not show warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# !pip install -U kaleido # w Google Colab wymagany Runtime restart po instalacji (Runtime -> Restart Runtime)\n",
    "import kaleido #required\n",
    "# kaleido.__version__ #0.2.1\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip\n",
    "# importing the \"tarfile\" module\n",
    "import tarfile # type: ignore\n",
    "\n",
    "# open file\n",
    "file = tarfile.open('2_150x9.tar.gz')\n",
    "\n",
    "# extracting a specific file\n",
    "file.extractall(path='./data/2_150x9/')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator\n",
    "def data(time, features):\n",
    "    # Timestep \n",
    "    # timestepsPerSample = 20\n",
    "    timestepsPerSample = time\n",
    "\n",
    "    # Timesteps in input data\n",
    "    timestepsPerSampleWholeData = 150\n",
    "\n",
    "    # Replace 'your_file.csv' with the actual file path\n",
    "    file_features = './data/2_150x9/2_150x9f.csv'\n",
    "    file_labels = './data/2_150x9/2_150x9l.csv'\n",
    "\n",
    "    # Read the .csv file and create an array\n",
    "    data_strings = np.genfromtxt(file_features, delimiter=';')\n",
    "    labels_strings = np.genfromtxt(file_labels,delimiter=';')\n",
    "\n",
    "\n",
    "    # Wycinanie wybranych kolumn\n",
    "    #  0    1         2             3        4       5        6    7        8\n",
    "    # RSI, VWAP, HeikenResult, closeHeiken, CMF, Stochastic, OBV, QQE, TrendFilter\n",
    "    # data_s = data_strings[:,[0,2,3,4,5,6,7,8]]\n",
    "    if features == 0:\n",
    "        data_s = data_strings[:,[0,4,5,8]]\n",
    "    elif features == 1:\n",
    "        data_s = data_strings[:,[0,1,4,5,6,8]]\n",
    "    elif features == 2:\n",
    "        data_s = data_strings[:,:]\n",
    "        \n",
    "    num_features = data_s.shape[1]\n",
    "    # print(data_strings[:3])\n",
    "    # print(data_s[:3])\n",
    "\n",
    "    \n",
    "    # Convert from strings to float and int\n",
    "    X = data_s.astype(float).reshape((-1,timestepsPerSampleWholeData,num_features))\n",
    "    Y = labels_strings.astype(float).reshape((-1,6))\n",
    "    # print(X.shape)\n",
    "    # print(Y.shape)\n",
    "\n",
    "    # Modification from imported to new size X[samples,timesteps]\n",
    "    X_mod = X[1000:11000,timestepsPerSampleWholeData - timestepsPerSample:]\n",
    "    Y_mod = Y[1000:11000]\n",
    "    timestepsPerSampleWholeData = X_mod.shape[1]\n",
    "    # print(X_mod.shape)\n",
    "    # print(Y_mod.shape)\n",
    "    # print(X_mod[:1])\n",
    "\n",
    "    # Splitting\n",
    "    x_train, x_test, Y_train, Y_test = train_test_split(X_mod,Y_mod, test_size=0.15, shuffle=False)\n",
    "    y_train = Y_train[:, 0:2]\n",
    "    y_test = Y_test[:, 0:2]\n",
    "    # print(y_train[:2])\n",
    "    # print(y_test[:2])\n",
    "\n",
    "    # Summarize\n",
    "    num_samples = x_train.shape[0]\n",
    "    test_samples = x_test.shape[0]\n",
    "    print(f\"Train shape: {x_train.shape}\")\n",
    "    print(f\"Timesteps: {timestepsPerSampleWholeData}\")\n",
    "    print(f\"Train Samples: {num_samples}\")\n",
    "    print(f\"Test Samples: {test_samples}\")\n",
    "    print(f\"Num features: {num_features}\")\n",
    "    return x_train, x_test, y_train, y_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit lstm model\n",
    "from tabnanny import verbose\n",
    "\n",
    "\n",
    "def fit_lstmModel(i, x_train, y_train, x_test, y_test, batch_size, nb_epoch, neurons, denseType, dropout, model, learning_rate):\n",
    "    if model == 0:\n",
    "        # Random seed\n",
    "        import numpy as np\n",
    "        np.random.seed(2016) \n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # Assuming `data` is your input matrix with shape (samples, time_steps, features)\n",
    "        model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(LSTM(units=50, return_sequences=False))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(units=25))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Output layer for price prediction\n",
    "        model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "    elif model == 1:\n",
    "        model = Sequential()\n",
    "\n",
    "        # Assuming `data` is your input matrix with shape (samples, time_steps, features)\n",
    "        model.add(LSTM(units=150, return_sequences=True, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(LSTM(units=150, return_sequences=False))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(units=75))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Output layer for price prediction\n",
    "        model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "    elif model == 2:\n",
    "        model = Sequential()\n",
    "\n",
    "        # Assuming `data` is your input matrix with shape (samples, time_steps, features)\n",
    "        model.add(LSTM(units=300, return_sequences=True, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(LSTM(units=300, return_sequences=False))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(units=150))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Output layer for price prediction\n",
    "        model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    num_samples = x_train.shape[0]\n",
    "    STEPS_PER_EPOCH = num_samples/batch_size\n",
    "\n",
    "    # lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    0.001,\n",
    "    decay_steps=STEPS_PER_EPOCH*100,\n",
    "    decay_rate=0.9,\n",
    "    staircase=False)\n",
    "\n",
    "    # Callback Checkpoint\n",
    "    checkpoint = ModelCheckpoint(\n",
    "            filepath='./saved_models/last_saved_model.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_loss',\n",
    "            verbose=1\n",
    "            )\n",
    "\n",
    "    # Callback EarlyStopping\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                                             start_from_epoch=10,\n",
    "                                             restore_best_weights=True,\n",
    "                                             verbose=0,\n",
    "                                             patience=3)\n",
    "\n",
    "    def get_optimizer():\n",
    "         # return tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        # return tf.keras.optimizers.Adam() # type: ignore\n",
    "        return tf.keras.optimizers.RMSprop(learning_rate=learning_rate) # type: ignore\n",
    "        # return tf.keras.optimizers.Adam(learning_rate= lr_schedule) # type: ignore\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "\n",
    "    # defining loss function, optimizer, metrics and then compiling model\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "    # Save initial weights on first compile. In next compile restore initial weights\n",
    "    if i == 0:\n",
    "        model.save_weights(\"./saved_models/initial.weights.h5\")\n",
    "    elif i > 0:\n",
    "        model.load_weights(\"./saved_models/initial.weights.h5\")\n",
    "        \n",
    "    model.summary()\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit(x_train, y_train, epochs=nb_epoch, batch_size=batch_size, \n",
    "                        shuffle=False, validation_data=(x_test, y_test), callbacks=[checkpoint, earlyStopping], verbose=2) # type: ignore\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profit\n",
    "def funcProfit(predict, Y_test):\n",
    "    predict_classes = np.where(predict > 0.5, 1,0)\n",
    "    concat = np.hstack((predict_classes, Y_test[:,2:]))\n",
    "    # df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Price'])\n",
    "    df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Close', 'Open', 'High', 'Low'])\n",
    "\n",
    "    # Absolute difference prices\n",
    "    spread = 0.03\n",
    "    tp = 0.20\n",
    "    sum = 0\n",
    "    sell = 0\n",
    "    buy = 0\n",
    "    for i in range(0, len(df)):\n",
    "        if (i-1) >= 0:\n",
    "            if df.at[i,'Sell'] > 0.8 and df.at[i-1,'Sell'] < 0.2:                  # S 0 1 \n",
    "                if buy > 0:\n",
    "                    sell = df.at[i,'Open'] - spread\n",
    "                    sum += df.at[i,'Open'] - buy\n",
    "                    buy = 0\n",
    "                elif sell == 0:\n",
    "                    sell = df.at[i,'Open'] - spread\n",
    "            elif df.at[i,'Sell'] > 0.8 and df.at[i-1,'Sell'] > 0.8 and sell > 0:   # S 1 1 sell >0\n",
    "                if df.at[i-1,'High'] >= (sell + tp):\n",
    "                    sum -= tp\n",
    "                    sell = 0\n",
    "                if df.at[i-1,'Low'] <= (sell - tp):\n",
    "                    sum += tp\n",
    "                    sell = 0\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            elif df.at[i,'Buy'] > 0.8 and df.at[i-1,'Buy'] < 0.2:               # B 0 1 \n",
    "                if sell > 0:\n",
    "                    buy = df.at[i,'Open'] + spread\n",
    "                    sum += sell - df.at[i,'Open']\n",
    "                    sell = 0\n",
    "                elif buy == 0:\n",
    "                    buy = df.at[i,'Open'] + spread\n",
    "            elif df.at[i,'Buy'] > 0.8 and df.at[i-1,'Buy'] > 0.8 and buy > 0:   # B 1 1 buy >0\n",
    "                if df.at[i-1,'Low'] <= (buy - tp):\n",
    "                    sum -= tp\n",
    "                    buy = 0\n",
    "                if df.at[i-1,'High'] >= (buy + tp):\n",
    "                    sum += tp\n",
    "                    buy = 0\n",
    "\n",
    "    return round(sum, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment\n",
    "def experiment(i, repeats, epochs, neurons, time, denseType, dropout, model, features, learning_rate):\n",
    "    # # Data gen\n",
    "    # x_train, x_test, y_train, y_test, Y_test = data(time, features)\n",
    "\n",
    "    # Evaluate declaration\n",
    "    accuracy = list()\n",
    "    profit = list()\n",
    "    metrics = list()\n",
    "\n",
    "    # Repeats\n",
    "    for r in range(repeats):\n",
    "        print(f\"Repeat {r} running...\")\n",
    "        # Batch size\n",
    "        batch_size = 64\n",
    "        # model\n",
    "        model, history = fit_lstmModel(i, x_train, y_train, x_test, y_test, batch_size, epochs, neurons, denseType, dropout, model, learning_rate)\n",
    "        i += 1\n",
    "        \n",
    "        # forecast the entire training dataset to build up state for forecasting\n",
    "        model = load_model(filepath=\"./saved_models/last_saved_model.keras\")\n",
    "        predict = model.predict(x_test, batch_size=batch_size)\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0) # type: ignore\n",
    "\n",
    "        metric = pd.DataFrame(history.history)\n",
    "        metric['epoch'] = history.epoch\n",
    "\n",
    "        metrics.append(metric)\n",
    "        accuracy.append(test_acc*100)\n",
    "        profit.append(funcProfit(predict, Y_test))\n",
    "    \n",
    "    return accuracy, profit, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "def plotsOut(d, metrics):\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=('Accuracy', 'Loss'))\n",
    "\n",
    "    # Layout - set size\n",
    "    fig.update_layout(\n",
    "        autosize=True,\n",
    "        width=1000\n",
    "    )\n",
    "\n",
    "    for idx, m in enumerate(metrics):\n",
    "        fig.add_trace(go.Scatter(x=m['epoch'], y=m['accuracy'], name=f'accuracy{str(idx)}', line_color='#0000ff', showlegend=False), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=m['epoch'], y=m['loss'], name=f'loss{str(idx)}', line_color='#0000ff', showlegend=False), row=1, col=2)\n",
    "        fig.add_trace(go.Scatter(x=m['epoch'], y=m['val_accuracy'], name=f'val_accuracy{str(idx)}', line_color='#EF8260', showlegend=False), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=m['epoch'], y=m['val_loss'], name=f'val_loss{str(idx)}', line_color='#EF8260', showlegend=False), row=1, col=2)\n",
    "\n",
    "    fig.update_xaxes(title_text='epochs')\n",
    "    fig.update_yaxes(title_text='')\n",
    "    # fig.update_layout(width=1000, title='Accuracy and Loss')\n",
    "    fig.write_image(file=f\"compare_models_{str(d)}.jpg\", engine=\"kaleido\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Data gen\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m x_train, x_test, y_train, y_test, Y_test \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeStep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# for dr in dropout:\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dr \u001b[38;5;129;01min\u001b[39;00m dropout:\n",
      "Cell \u001b[1;32mIn[15], line 15\u001b[0m, in \u001b[0;36mdata\u001b[1;34m(time, features)\u001b[0m\n\u001b[0;32m     12\u001b[0m file_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/2_150x9/2_150x9l.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Read the .csv file and create an array\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m data_strings \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenfromtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m labels_strings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgenfromtxt(file_labels,delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Wycinanie wybranych kolumn\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#  0    1         2             3        4       5        6    7        8\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# RSI, VWAP, HeikenResult, closeHeiken, CMF, Stochastic, OBV, QQE, TrendFilter\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# data_s = data_strings[:,[0,2,3,4,5,6,7,8]]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\.conda\\envs\\tf_env\\lib\\site-packages\\numpy\\lib\\npyio.py:2247\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[0;32m   2244\u001b[0m append_to_invalid \u001b[38;5;241m=\u001b[39m invalid\u001b[38;5;241m.\u001b[39mappend\n\u001b[0;32m   2246\u001b[0m \u001b[38;5;66;03m# Parse each line\u001b[39;00m\n\u001b[1;32m-> 2247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (i, line) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mchain([first_line, ], fhd)):\n\u001b[0;32m   2248\u001b[0m     values \u001b[38;5;241m=\u001b[39m split_line(line)\n\u001b[0;32m   2249\u001b[0m     nbvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(values)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\.conda\\envs\\tf_env\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run\n",
    "from numpy import dtype\n",
    "\n",
    "repeats = 4\n",
    "runMeter = 0\n",
    "results_acc = DataFrame(dtype='float')\n",
    "results_profit = DataFrame(dtype='float')\n",
    "metrics = list()\n",
    "\n",
    "# vary training epochs\n",
    "learning_rate = 0.001\n",
    "features = 0\n",
    "model = [0, 1, 2]\n",
    "dropout = 0 # [0.2, 0.4, 0.6, 0.8]\n",
    "denseType = 0\n",
    "neuronsLstm = 150\n",
    "timeStep = 150\n",
    "epochs = 100\n",
    "\n",
    "# Data gen\n",
    "x_train, x_test, y_train, y_test, Y_test = data(timeStep, features)\n",
    "\n",
    "# for dr in dropout:\n",
    "for mod in model:\n",
    " print(f\"{mod} Model test start...\")\n",
    " \n",
    " results_acc[str(mod)], results_profit[str(mod)], metrics = experiment(runMeter, repeats, epochs, neuronsLstm, timeStep, denseType, dropout, mod, features, learning_rate)\n",
    "#  runMeter += 1\n",
    " plotsOut(mod, metrics)\n",
    "\n",
    "# summarize results\n",
    "print(results_acc.describe())\n",
    "print(results_profit.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0.2        0.4        0.6        0.8\n",
      "count   1.000000   1.000000   1.000000   1.000000\n",
      "mean   63.266665  63.800001  63.266665  63.666666\n",
      "std          NaN        NaN        NaN        NaN\n",
      "min    63.266665  63.800001  63.266665  63.666666\n",
      "25%    63.266665  63.800001  63.266665  63.666666\n",
      "50%    63.266665  63.800001  63.266665  63.666666\n",
      "75%    63.266665  63.800001  63.266665  63.666666\n",
      "max    63.266665  63.800001  63.266665  63.666666\n",
      "        0.2   0.4   0.6   0.8\n",
      "count  1.00  1.00  1.00  1.00\n",
      "mean   0.03 -1.38 -0.74 -0.03\n",
      "std     NaN   NaN   NaN   NaN\n",
      "min    0.03 -1.38 -0.74 -0.03\n",
      "25%    0.03 -1.38 -0.74 -0.03\n",
      "50%    0.03 -1.38 -0.74 -0.03\n",
      "75%    0.03 -1.38 -0.74 -0.03\n",
      "max    0.03 -1.38 -0.74 -0.03\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(results_acc.describe())\n",
    "print(results_profit.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(results_acc.describe())\n",
    "\n",
    "# 2 subplots in one row\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "# Generate boxplots\n",
    "results_acc.boxplot(ax=ax[0])\n",
    "results_profit.boxplot(ax=ax[1])\n",
    "\n",
    "# Set labels and titles\n",
    "ax[0].set_title('Accuracy')\n",
    "ax[0].set_xlabel('Model')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Profit')\n",
    "ax[1].set_xlabel('Model')\n",
    "ax[1].set_ylabel('Profit')\n",
    "\n",
    "# Save to .png and show plot\n",
    "plt.savefig(f'boxplot_model.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
