{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intallation packages before use Jupyter\n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install keras\n",
    "# !pip install matplotlib\n",
    "# !pip install kaleido\n",
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "# from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.metrics import Precision\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import schedules\n",
    "from math import sqrt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "# be able to save images on server\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Not show warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# !pip install -U kaleido # w Google Colab wymagany Runtime restart po instalacji (Runtime -> Restart Runtime)\n",
    "# import kaleido #required\n",
    "# kaleido.__version__ #0.2.1\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip\n",
    "# importing the \"tarfile\" module\n",
    "import tarfile # type: ignore\n",
    "\n",
    "# open file\n",
    "file = tarfile.open('2_150x9.tar.gz')\n",
    "\n",
    "# extracting a specific file\n",
    "file.extractall(path='./data/2_150x9/')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator\n",
    "def data(time, features):\n",
    "    # Timestep \n",
    "    # timestepsPerSample = 20\n",
    "    timestepsPerSample = time\n",
    "\n",
    "    # Timesteps in input data\n",
    "    timestepsPerSampleWholeData = 150\n",
    "\n",
    "    # Replace 'your_file.csv' with the actual file path\n",
    "    file_features = './data/2_150x9/2_150x9f.csv'\n",
    "    file_labels = './data/2_150x9/2_150x9l.csv'\n",
    "\n",
    "    # Read the .csv file and create an array\n",
    "    data_strings = np.genfromtxt(file_features, delimiter=';')\n",
    "    labels_strings = np.genfromtxt(file_labels,delimiter=';')\n",
    "\n",
    "\n",
    "    # Wycinanie wybranych kolumn\n",
    "    #  0    1         2             3        4       5        6    7        8\n",
    "    # RSI, VWAP, HeikenResult, closeHeiken, CMF, Stochastic, OBV, QQE, TrendFilter\n",
    "    # data_s = data_strings[:,[0,2,3,4,5,6,7,8]]\n",
    "    if features == 0:\n",
    "        data_s = data_strings[:,[0,4,5,8]]\n",
    "    elif features == 1:\n",
    "        data_s = data_strings[:,[0,1,4,5,6,8]]\n",
    "    elif features == 2:\n",
    "        data_s = data_strings[:,:]\n",
    "        \n",
    "    num_features = data_s.shape[1]\n",
    "    # print(data_strings[:3])\n",
    "    # print(data_s[:3])\n",
    "\n",
    "    \n",
    "    # Convert from strings to float and int\n",
    "    X = data_s.astype(float).reshape((-1,timestepsPerSampleWholeData,num_features))\n",
    "    Y = labels_strings.astype(float).reshape((-1,6))\n",
    "    # print(X.shape)\n",
    "    # print(Y.shape)\n",
    "\n",
    "    # Modification from imported to new size X[samples,timesteps]\n",
    "    X_mod = X[1000:11000,timestepsPerSampleWholeData - timestepsPerSample:]\n",
    "    Y_mod = Y[1000:11000]\n",
    "    timestepsPerSampleWholeData = X_mod.shape[1]\n",
    "    # print(X_mod.shape)\n",
    "    # print(Y_mod.shape)\n",
    "    # print(X_mod[:1])\n",
    "\n",
    "    # Splitting\n",
    "    x_train, x_test, Y_train, Y_test = train_test_split(X_mod,Y_mod, test_size=0.15, shuffle=False)\n",
    "    y_train = Y_train[:, 0:2]\n",
    "    y_test = Y_test[:, 0:2]\n",
    "    # print(y_train[:2])\n",
    "    # print(y_test[:2])\n",
    "\n",
    "    # Summarize\n",
    "    num_samples = x_train.shape[0]\n",
    "    test_samples = x_test.shape[0]\n",
    "    print(f\"Train shape: {x_train.shape}\")\n",
    "    print(f\"Timesteps: {timestepsPerSampleWholeData}\")\n",
    "    print(f\"Train Samples: {num_samples}\")\n",
    "    print(f\"Test Samples: {test_samples}\")\n",
    "    print(f\"Num features: {num_features}\")\n",
    "    return x_train, x_test, y_train, y_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit lstm model\n",
    "from tabnanny import verbose\n",
    "\n",
    "def shuffle_weights(model, weights=None):\n",
    "    \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
    "    This is a fast approximation of re-initializing the weights of a model.\n",
    "    Assumes weights are distributed independently of the dimensions of the weight tensors\n",
    "      (i.e., the weights have the same distribution along each dimension).\n",
    "    :param Model model: Modify the weights of the given model.\n",
    "    :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
    "      If `None`, permute the model's current weights.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    # Faster, but less random: only permutes along the first dimension\n",
    "    # weights = [np.random.permutation(w) for w in weights]\n",
    "    model.set_weights(weights)\n",
    "\n",
    "\n",
    "def fit_lstmModel(i, x_train, y_train, x_test, y_test, batch_size, nb_epoch, neurons, denseType, dropout, model, learning_rate):\n",
    "    LstmLayer = LSTM(\n",
    "    units=neurons,\n",
    "    activation=\"tanh\",\n",
    "    recurrent_activation=\"sigmoid\",\n",
    "    use_bias=True, # true if cuDNN\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    recurrent_initializer=\"orthogonal\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    unit_forget_bias=True,\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    dropout=dropout, # !important parameter for optimization => 0 if cuDNN\n",
    "    recurrent_dropout=0.0,\n",
    "    seed=None,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    "    unroll=False, # false if cuDNN\n",
    "    input_shape=(x_train.shape[1],x_train.shape[2]),\n",
    "    # input_dim=(x_train.shape[1])\n",
    "    ) \n",
    "\n",
    "    if model == 0:\n",
    "        model = Sequential() # initializing model\n",
    "        # input layer and LSTM layer with 50 neurons\n",
    "        model.add(LstmLayer)\n",
    "        # model.add(Dense(100, activation='relu'))\n",
    "        # model.add(Dense(100, activation='relu'))\n",
    "        # model.add(Dense(20, activation='relu'))\n",
    "        # outpute layer with sigmoid activation\n",
    "        model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "    if model == 1:\n",
    "        model = Sequential()\n",
    "\n",
    "        # Assuming `data` is your input matrix with shape (samples, time_steps, features)\n",
    "        model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(LSTM(units=50, return_sequences=False))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(units=25))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Output layer for price prediction\n",
    "        model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "    elif model == 2:\n",
    "        model = Sequential()\n",
    "\n",
    "        # Assuming `data` is your input matrix with shape (samples, time_steps, features)\n",
    "        model.add(LSTM(units=150, return_sequences=True, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(LSTM(units=150, return_sequences=False))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(units=75))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Output layer for price prediction\n",
    "        model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "    elif model == 3:\n",
    "        model = Sequential()\n",
    "\n",
    "        # Assuming `data` is your input matrix with shape (samples, time_steps, features)\n",
    "        model.add(LSTM(units=300, return_sequences=True, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(LSTM(units=300, return_sequences=False))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(units=150))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Output layer for price prediction\n",
    "        model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    num_samples = x_train.shape[0]\n",
    "    STEPS_PER_EPOCH = num_samples/batch_size\n",
    "\n",
    "    lr_schedule = schedules.InverseTimeDecay(\n",
    "    # lr_schedule = schedules.ExponentialDecay(\n",
    "    0.001,\n",
    "    decay_steps=STEPS_PER_EPOCH*100,\n",
    "    decay_rate=0.9,\n",
    "    staircase=False)\n",
    "\n",
    "    # Callback Checkpoint\n",
    "    checkpoint = ModelCheckpoint(\n",
    "            filepath='./saved_models/last_saved_model.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_loss',\n",
    "            verbose=1\n",
    "            )\n",
    "\n",
    "    # Callback EarlyStopping\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                                             start_from_epoch=80,\n",
    "                                             restore_best_weights=True,\n",
    "                                             verbose=0,\n",
    "                                             patience=3)\n",
    "\n",
    "    def get_optimizer():\n",
    "         # return tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        # return tf.keras.optimizers.Adam() # type: ignore\n",
    "        # return tf.keras.optimizers.RMSprop(learning_rate=learning_rate) # type: ignore\n",
    "        return tf.keras.optimizers.Adam(learning_rate= lr_schedule) # type: ignore\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "\n",
    "    # defining loss function, optimizer, metrics and then compiling model\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['precision'])\n",
    "\n",
    "    # Save initial weights on first compile. In next compile restore initial weights\n",
    "    if i == 0:\n",
    "        model.save_weights(\"./saved_models/initial.weights.h5\")\n",
    "    elif i > 0:\n",
    "        model.shuffle_weights(model, weights=None)\n",
    "        \n",
    "    model.summary()\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit(x_train, y_train, epochs=nb_epoch, batch_size=batch_size, \n",
    "                        shuffle=False, validation_data=(x_test, y_test), callbacks=[checkpoint, earlyStopping], verbose=2) # type: ignore\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profit old\n",
    "def funcProfitOld(predict, Y_test):\n",
    "    predict_classes = np.where(predict > 0.5, 1,0)\n",
    "    concat = np.hstack((predict_classes, Y_test[:,2:]))\n",
    "    df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Price'])\n",
    "\n",
    "    # Absolute difference prices\n",
    "    sum = 0\n",
    "    for i in range(0, len(df)):\n",
    "        if (i-1) > 0:\n",
    "            diff = abs((df.at[i,'Price'] - df.at[i-1,'Price']))\n",
    "            if diff < 0.5:\n",
    "                if df.at[i,'Sell'] == 1:\n",
    "                    sum += (df.at[i,'Price'] - df.at[i-1,'Price'])*(-1)\n",
    "                elif df.at[i,'Buy'] == 1:\n",
    "                    sum += (df.at[i,'Price'] - df.at[i-1,'Price'])\n",
    "                if df.at[i,'Sell'] != df.at[i-1,'Sell']:\n",
    "                    sum -= 0.03\n",
    "    return sum\n",
    "\n",
    "\n",
    "# Profit\n",
    "def funcProfit(predict, Y_test):\n",
    "    predict_classes = np.where(predict > 0.5, 1,0)\n",
    "    concat = np.hstack((predict_classes, Y_test[:,2:]))\n",
    "    # df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Price'])\n",
    "    df = pd.DataFrame(concat, columns=['Sell', 'Buy', 'Close', 'Open', 'High', 'Low'])\n",
    "\n",
    "    # Absolute difference prices\n",
    "    spread = 0.03\n",
    "    tp = 0.20\n",
    "    sum = 0\n",
    "    sell = 0\n",
    "    buy = 0\n",
    "    for i in range(0, len(df)):\n",
    "        if (i-1) >= 0:\n",
    "            if df.at[i,'Sell'] > 0.8 and df.at[i-1,'Sell'] < 0.2:                  # S 0 1 \n",
    "                if buy > 0:\n",
    "                    sell = df.at[i,'Open'] - spread\n",
    "                    sum += df.at[i,'Open'] - buy\n",
    "                    buy = 0\n",
    "                elif sell == 0:\n",
    "                    sell = df.at[i,'Open'] - spread\n",
    "            elif df.at[i,'Sell'] > 0.8 and df.at[i-1,'Sell'] > 0.8 and sell > 0:   # S 1 1 sell >0\n",
    "                if df.at[i-1,'High'] >= (sell + tp):\n",
    "                    sum -= tp\n",
    "                    sell = 0\n",
    "                if df.at[i-1,'Low'] <= (sell - tp):\n",
    "                    sum += tp\n",
    "                    sell = 0\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            elif df.at[i,'Buy'] > 0.8 and df.at[i-1,'Buy'] < 0.2:               # B 0 1 \n",
    "                if sell > 0:\n",
    "                    buy = df.at[i,'Open'] + spread\n",
    "                    sum += sell - df.at[i,'Open']\n",
    "                    sell = 0\n",
    "                elif buy == 0:\n",
    "                    buy = df.at[i,'Open'] + spread\n",
    "            elif df.at[i,'Buy'] > 0.8 and df.at[i-1,'Buy'] > 0.8 and buy > 0:   # B 1 1 buy >0\n",
    "                if df.at[i-1,'Low'] <= (buy - tp):\n",
    "                    sum -= tp\n",
    "                    buy = 0\n",
    "                if df.at[i-1,'High'] >= (buy + tp):\n",
    "                    sum += tp\n",
    "                    buy = 0\n",
    "\n",
    "    return round(sum, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment\n",
    "def experiment(i, repeats, epochs, neurons, time, denseType, dropout, model, features, learning_rate):\n",
    "    # # Data gen\n",
    "    # x_train, x_test, y_train, y_test, Y_test = data(time, features)\n",
    "\n",
    "    # Evaluate declaration\n",
    "    accuracy = list()\n",
    "    profit = list()\n",
    "    profitOld = list()\n",
    "    metrics = list()\n",
    "\n",
    "    # Repeats\n",
    "    for r in range(repeats):\n",
    "        print(f\"Repeat {r} running...\")\n",
    "        # Batch size\n",
    "        batch_size = 64\n",
    "        # model\n",
    "        model, history = fit_lstmModel(i, x_train, y_train, x_test, y_test, batch_size, epochs, neurons, denseType, dropout, model, learning_rate)\n",
    "        i += 1\n",
    "        \n",
    "        # forecast the entire training dataset to build up state for forecasting\n",
    "        model = load_model(filepath=\"./saved_models/last_saved_model.keras\")\n",
    "        predict = model.predict(x_test, batch_size=batch_size)\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0) # type: ignore\n",
    "\n",
    "        metric = pd.DataFrame(history.history)\n",
    "        metric['epoch'] = history.epoch\n",
    "\n",
    "        metrics.append(metric)\n",
    "        accuracy.append(test_acc*100)\n",
    "        profit.append(funcProfit(predict, Y_test))\n",
    "        profitOld.append(funcProfitOld(predict, Y_test))\n",
    "    \n",
    "    return accuracy, profit, profitOld, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "def plotsOut(d, metrics):\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=('Precision', 'Loss'))\n",
    "\n",
    "    # Layout - set size\n",
    "    fig.update_layout(\n",
    "        autosize=True,\n",
    "        width=1000\n",
    "    )\n",
    "\n",
    "    for idx, m in enumerate(metrics):\n",
    "        fig.add_trace(go.Scatter(x=m['epoch'], y=m['precision'], name=f'precision{str(idx)}', line_color='#0000ff', showlegend=False), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=m['epoch'], y=m['loss'], name=f'loss{str(idx)}', line_color='#0000ff', showlegend=False), row=1, col=2)\n",
    "        fig.add_trace(go.Scatter(x=m['epoch'], y=m['val_precision'], name=f'val_precision{str(idx)}', line_color='#EF8260', showlegend=False), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=m['epoch'], y=m['val_loss'], name=f'val_loss{str(idx)}', line_color='#EF8260', showlegend=False), row=1, col=2)\n",
    "\n",
    "    fig.update_xaxes(title_text='epochs')\n",
    "    fig.update_yaxes(title_text='')\n",
    "    # fig.update_layout(width=1000, title='Accuracy and Loss')\n",
    "    fig.write_image(file=f\"compare_models_{str(d)}.jpg\", engine=\"kaleido\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "./data/2_150x9/2_150x9f.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Data gen\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m x_train, x_test, y_train, y_test, Y_test \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeStep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# for dr in dropout:\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dr \u001b[38;5;129;01min\u001b[39;00m dropout:\n",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m, in \u001b[0;36mdata\u001b[1;34m(time, features)\u001b[0m\n\u001b[0;32m     12\u001b[0m file_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/2_150x9/2_150x9l.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Read the .csv file and create an array\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m data_strings \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenfromtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m labels_strings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgenfromtxt(file_labels,delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Wycinanie wybranych kolumn\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#  0    1         2             3        4       5        6    7        8\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# RSI, VWAP, HeikenResult, closeHeiken, CMF, Stochastic, OBV, QQE, TrendFilter\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# data_s = data_strings[:,[0,2,3,4,5,6,7,8]]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kucha\\.conda\\envs\\tf\\lib\\site-packages\\numpy\\lib\\npyio.py:1980\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[0;32m   1978\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os_fspath(fname)\n\u001b[0;32m   1979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1980\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1981\u001b[0m     fid_ctx \u001b[38;5;241m=\u001b[39m contextlib\u001b[38;5;241m.\u001b[39mclosing(fid)\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kucha\\.conda\\envs\\tf\\lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kucha\\.conda\\envs\\tf\\lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: ./data/2_150x9/2_150x9f.csv not found."
     ]
    }
   ],
   "source": [
    "# Run\n",
    "\n",
    "repeats = 1\n",
    "runMeter = 0\n",
    "results_acc = DataFrame(dtype='float')\n",
    "results_profit = DataFrame(dtype='float')\n",
    "results_profitOld = DataFrame(dtype='float')\n",
    "metrics = list()\n",
    "\n",
    "# vary training epochs\n",
    "learning_rate = 0.001\n",
    "features = 0\n",
    "model = 0 # [0, 1, 2]\n",
    "dropout = [0.2, 0.4, 0.6, 0.8]\n",
    "denseType = 0\n",
    "neuronsLstm = 150\n",
    "timeStep = 150\n",
    "epochs = 1\n",
    "\n",
    "# Data gen\n",
    "x_train, x_test, y_train, y_test, Y_test = data(timeStep, features)\n",
    "\n",
    "# for dr in dropout:\n",
    "for dr in dropout:\n",
    " print(f\"{dr} Model test start...\")\n",
    " \n",
    " results_acc[str(dr)], results_profit[str(dr)], results_profitOld[str(dr)], metrics = experiment(\n",
    "                                                                                            runMeter, \n",
    "                                                                                            repeats, \n",
    "                                                                                            epochs, \n",
    "                                                                                            neuronsLstm, \n",
    "                                                                                            timeStep, \n",
    "                                                                                            denseType, \n",
    "                                                                                            dr, \n",
    "                                                                                            model, \n",
    "                                                                                            features, \n",
    "                                                                                            learning_rate\n",
    "                                                                                            )\n",
    "                          \n",
    "#  runMeter += 1\n",
    " plotsOut(dr, metrics)\n",
    "\n",
    "# summarize results\n",
    "print(results_acc.describe())\n",
    "print(results_profit.describe())\n",
    "print(results_profitOld.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0.2        0.4        0.6        0.8\n",
      "count   1.000000   1.000000   1.000000   1.000000\n",
      "mean   63.266665  63.800001  63.266665  63.666666\n",
      "std          NaN        NaN        NaN        NaN\n",
      "min    63.266665  63.800001  63.266665  63.666666\n",
      "25%    63.266665  63.800001  63.266665  63.666666\n",
      "50%    63.266665  63.800001  63.266665  63.666666\n",
      "75%    63.266665  63.800001  63.266665  63.666666\n",
      "max    63.266665  63.800001  63.266665  63.666666\n",
      "        0.2   0.4   0.6   0.8\n",
      "count  1.00  1.00  1.00  1.00\n",
      "mean   0.03 -1.38 -0.74 -0.03\n",
      "std     NaN   NaN   NaN   NaN\n",
      "min    0.03 -1.38 -0.74 -0.03\n",
      "25%    0.03 -1.38 -0.74 -0.03\n",
      "50%    0.03 -1.38 -0.74 -0.03\n",
      "75%    0.03 -1.38 -0.74 -0.03\n",
      "max    0.03 -1.38 -0.74 -0.03\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(results_acc.describe())\n",
    "print(results_profit.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(results_acc.describe())\n",
    "\n",
    "# 2 subplots in one row\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 6))\n",
    "\n",
    "# Generate boxplots\n",
    "results_acc.boxplot(ax=ax[0])\n",
    "results_profit.boxplot(ax=ax[1])\n",
    "results_profitOld.boxplot(ax=ax[2])\n",
    "\n",
    "# Set labels and titles\n",
    "ax[0].set_title('Accuracy')\n",
    "ax[0].set_xlabel('Model')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Profit')\n",
    "ax[1].set_xlabel('Model')\n",
    "ax[1].set_ylabel('Profit')\n",
    "ax[2].set_title('ProfitOld')\n",
    "ax[2].set_xlabel('Model')\n",
    "ax[2].set_ylabel('ProfitOld')\n",
    "\n",
    "# Save to .png and show plot\n",
    "plt.savefig(f'boxplot_dropout.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
